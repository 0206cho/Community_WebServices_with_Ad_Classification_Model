## ê´‘ê³  ë¶„ë¥˜ ëª¨ë¸ì„ ë„ì…í•œ ì»¤ë®¤ë‹ˆí‹° ì›¹ ì„œë¹„ìŠ¤<br>_Community Web Services with Ad Classification Model
<p> 
 
<img src="https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=Python&logoColor=white"/>
<img src="https://img.shields.io/badge/HTML5-E34F26?style=flat-square&logo=HTML5&logoColor=white"/>
<img src="https://img.shields.io/badge/CSS3-1572B6?style=flat-square&logo=CSS3&logoColor=white"/>
<img src="https://img.shields.io/badge/JavaScript-F7DF1E?style=flat-square&logo=JavaScript&logoColor=white"/>
</p>
<hr>

## ğŸ“‘ í”„ë¡œì íŠ¸ ê°œìš”
- ì–¸íƒíŠ¸ ì†Œë¹„ê°€ í™œì„±í™”ë˜ë©´ì„œ ë”ìš± ë§ì€ ì†Œë¹„ìë“¤ì´ ìƒí’ˆí‰ì´ë‚˜ êµ¬ë§¤ í›„ê¸° ë“±ì— ì˜ì¡´í•˜ê²Œ ë˜ê³ , ì´ì— ë”°ë¼ ê¸°ì—…ë“¤ì€ êµ¬ë§¤í›„ê¸° ì‘ì„±ì— ë”°ë¥¸ í˜œíƒì„ ëŠ˜ë ¤ê°€ê³  ìˆë‹¤. <br>
- ê´‘ê³  ê¸°ìˆ , ì¦‰ ì• ë“œí…Œí¬(Ad-tech)ê°€ ë°œì „í•¨ì— ë”°ë¼ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ìƒˆë¡œìš´ ê´‘ê³ ë“¤ì´ ì´ìš©ìê°€ ì¶”êµ¬í•˜ëŠ” ì •ë³´/ì½˜í…ì¸ ì™€ êµ¬ë³„ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ìš©ìë“¤ì€ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆë‹¤.
- ë³¸ í”„ë¡œì íŠ¸ì˜ ëª©í‘œëŠ” KoBERTë¥¼ ì´ìš©í•´ ê´‘ê³  ë¶„ë¥˜ ëª¨ë¸ì„ ë„ì…í•œ ì»¤ë®¤ë‹ˆí‹° ì›¹ ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì´ë‹¤.
<br>

## ğŸ“‹ ì‹œìŠ¤í…œêµ¬ì„± ë° ê¸°ëŠ¥
### [ ì‹œìŠ¤í…œ êµ¬ì¡° ] <br>
<img src="imgs/image.png">

### [ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° ] <br>
<img src="imgs/md_db.png">

### [ ë™ì‘ ì‹œë‚˜ë¦¬ì˜¤ ] <br>
<img src="imgs/md_act.png">

### [ ëª¨ë¸ í•™ìŠµ ê²°ê³¼ ]  ê·¸ë˜í”„ : ì—í¬í¬ì— ë”°ë¥¸ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ì…‹ì˜ ì •í™•ë„  <br>
<img src="imgs/md_model.png">

í•™ìŠµ ë°ì´í„°ì…‹ì˜ ê²½ìš° ì—í¬í¬ê°€ 2ì¼ ë•Œ ê°€ì¥ ë†’ì€ ì ìˆ˜ê°€ ë‚˜ì™”ìœ¼ë‚˜, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì˜ ì •í™•ë„ì™€ í° ì°¨ì´ë¥¼ ë³´ì˜€ë‹¤. <br>
ì—í¬í¬ê°€ 7ì˜ ê²½ìš°, í•™ìŠµ ë°ì´í„°ì…‹ì€ 0.81511, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ 0.70770ìœ¼ë¡œ 0.10741 ì˜¤ì°¨ë¥¼ ë³´ì˜€ë‹¤.<br>
ë˜í•œ ì—í¬í¬ 10ì˜ ê²½ìš°ì—ëŠ”, í•™ìŠµ ë°ì´í„°ì…‹ 0.84204, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ 0.71208ìœ¼ë¡œ 0.12996 ì˜¤ì°¨ë¥¼ ë³´ì˜€ë‹¤. <br>
ë”°ë¼ì„œ ì—í¬í¬ê°€ 7 ë˜ëŠ” 10ì¼ë•Œì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì í•©í•˜ë‹¤ê³  ìƒê°í•´ 10ê¹Œì§€ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
<br>

## ğŸ’» í”„ë¡œì íŠ¸ êµ¬í˜„
<img src="imgs/md_img_1.png">
ì„œë¹„ìŠ¤ì— ì ‘ì†í•˜ë©´ ê²Œì‹œê¸€ì„ ë³¼ ìˆ˜ ìˆë‹¤. ê´‘ê³  ê¸€ì˜ ê²½ìš° ì œëª© ìœ„ì— ê´‘ê³  ë¼ë²¨ì´ í‘œì‹œëœë‹¤. <br> <br>
<img src="imgs/md_img_2.png">
ê¸€ì“°ê¸° ë²„íŠ¼ì„ ëˆŒëŸ¬ ê¸€ì„ ë“±ë¡í•  ìˆ˜ ìˆë‹¤. ë¨¼ì € ì¼ìƒ ê¸€ì„ ì…ë ¥í•˜ì˜€ë‹¤. <br> <br>
<img src="imgs/md_img_3.png">
ê¸€ì´ ë“±ë¡ëœ ëª¨ìŠµì´ë‹¤. ì¼ìƒ ê¸€ì´ê¸° ë•Œë¬¸ì— ê´‘ê³  ë¼ë²¨ì´ ì—†ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. <Br> <br>
<img src="imgs/md_img_4.png">
ëŒ“ê¸€ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ëŒ“ê¸€ì„ í™•ì¸ ë° ë“±ë¡í•  ìˆ˜ ìˆë‹¤.<br> <br>
<img src="imgs/md_img_5.png">
ê´‘ê³ ì„± ê¸€ì„ ë“±ë¡í•˜ì˜€ë‹¤. <br> <br>
<img src="imgs/md_img_6.png">
ëª¨ë¸ì´ í•´ë‹¹ ê¸€ì„ ê´‘ê³ ë¡œ ë¶„ë¥˜í•˜ì˜€ìœ¼ë©°, ë”°ë¼ì„œ ì œëª© ìœ„ì— ê´‘ê³  ë¼ë²¨ì´ í‘œì‹œë˜ì—ˆë‹¤. <br> <br>

### [ êµ¬í˜„ ì˜ìƒ ]
<img src="imgs/êµ¬í˜„ì˜ìƒ.gif">

<br>
<hr>

# KoBERT

* [KoBERT](#kobert)
  * [Korean BERT pre-trained cased (KoBERT)](#korean-bert-pre-trained-cased-kobert)
    * [Why'?'](#why)
    * [Training Environment](#training-environment)
    * [Requirements](#requirements)
    * [How to install](#how-to-install)
  * [How to use](#how-to-use)
    * [Using with PyTorch](#using-with-pytorch)
    * [Using with ONNX](#using-with-onnx)
    * [Using with MXNet-Gluon](#using-with-mxnet-gluon)
    * [Tokenizer](#tokenizer)
  * [Subtasks](#subtasks)
    * [Naver Sentiment Analysis](#naver-sentiment-analysis)
    * [KoBERTì™€ CRFë¡œ ë§Œë“  í•œêµ­ì–´ ê°ì²´ëª…ì¸ì‹ê¸°](#kobertì™€-crfë¡œ-ë§Œë“ -í•œêµ­ì–´-ê°ì²´ëª…ì¸ì‹ê¸°)
    * [Korean Sentence BERT](#korean-sentence-bert)
  * [Release](#release)
  * [Contacts](#contacts)
  * [License](#license)

---

## Korean BERT pre-trained cased (KoBERT)

### Why'?'

* êµ¬ê¸€ [BERT base multilingual cased](https://github.com/google-research/bert/blob/master/multilingual.md)ì˜ í•œêµ­ì–´ ì„±ëŠ¥ í•œê³„

### Training Environment

* Architecture

```python
predefined_args = {
        'attention_cell': 'multi_head',
        'num_layers': 12,
        'units': 768,
        'hidden_size': 3072,
        'max_length': 512,
        'num_heads': 12,
        'scaled': True,
        'dropout': 0.1,
        'use_residual': True,
        'embed_size': 768,
        'embed_dropout': 0.1,
        'token_type_vocab_size': 2,
        'word_embed': None,
    }
```

* í•™ìŠµì…‹

| ë°ì´í„°      | ë¬¸ì¥ | ë‹¨ì–´ |
| ----------- | ---- | ---- |
| í•œêµ­ì–´ ìœ„í‚¤ | 5M   | 54M  |

* í•™ìŠµ í™˜ê²½
  * V100 GPU x 32, Horovod(with InfiniBand)

![2019-04-29 í…ì„œë³´ë“œ ë¡œê·¸](imgs/2019-04-29_TensorBoard.png)

* ì‚¬ì „(Vocabulary)
  * í¬ê¸° : 8,002
  * í•œê¸€ ìœ„í‚¤ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œ í† í¬ë‚˜ì´ì €(SentencePiece)
  * Less number of parameters(92M < 110M )

### Requirements

* see [requirements.txt](https://github.com/SKTBrain/KoBERT/blob/master/requirements.txt)

### How to install

* Install KoBERT as a python package

  ```sh
  pip install git+https://git@github.com/SKTBrain/KoBERT.git@master
  ```

* If you want to modify source codes, please clone this repository

  ```sh
  git clone https://github.com/SKTBrain/KoBERT.git
  cd KoBERT
  pip install -r requirements.txt
  ```

---

## How to use

### Using with PyTorch

*Huggingface transformers APIê°€ í¸í•˜ì‹  ë¶„ì€ [ì—¬ê¸°](kobert_hf)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.*

```python
>>> import torch
>>> from kobert import get_pytorch_kobert_model
>>> input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])
>>> input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])
>>> token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])
>>> model, vocab  = get_pytorch_kobert_model()
>>> sequence_output, pooled_output = model(input_ids, input_mask, token_type_ids)
>>> pooled_output.shape
torch.Size([2, 768])
>>> vocab
Vocab(size=8002, unk="[UNK]", reserved="['[MASK]', '[SEP]', '[CLS]']")
>>> # Last Encoding Layer
>>> sequence_output[0]
tensor([[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],
        [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],
        [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],
       grad_fn=<SelectBackward>)
```

`model`ì€ ë””í´íŠ¸ë¡œ `eval()`ëª¨ë“œë¡œ ë¦¬í„´ë¨, ë”°ë¼ì„œ í•™ìŠµ ìš©ë„ë¡œ ì‚¬ìš©ì‹œ `model.train()`ëª…ë ¹ì„ í†µí•´ í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½í•  í•„ìš”ê°€ ìˆë‹¤.

* Naver Sentiment Analysis Fine-Tuning with pytorch
  * Colabì—ì„œ [ëŸ°íƒ€ì„] - [ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½] - í•˜ë“œì›¨ì–´ ê°€ì†ê¸°(GPU) ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
  * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb)

### Using with ONNX

```python
>>> import onnxruntime
>>> import numpy as np
>>> from kobert import get_onnx_kobert_model
>>> onnx_path = get_onnx_kobert_model()
>>> sess = onnxruntime.InferenceSession(onnx_path)
>>> input_ids = [[31, 51, 99], [15, 5, 0]]
>>> input_mask = [[1, 1, 1], [1, 1, 0]]
>>> token_type_ids = [[0, 0, 1], [0, 1, 0]]
>>> len_seq = len(input_ids[0])
>>> pred_onnx = sess.run(None, {'input_ids':np.array(input_ids),
>>>                             'token_type_ids':np.array(token_type_ids),
>>>                             'input_mask':np.array(input_mask),
>>>                             'position_ids':np.array(range(len_seq))})
>>> # Last Encoding Layer
>>> pred_onnx[-2][0]
array([[-0.24610452,  0.24282141,  0.25895312, ..., -0.48613444,
        -0.07305173,  0.07560554],
       [-0.24783179,  0.24200465,  0.25520486, ..., -0.4877185 ,
        -0.0727044 ,  0.07536091],
       [-0.24721591,  0.24196623,  0.2560626 , ..., -0.48743123,
        -0.07326943,  0.07650235]], dtype=float32)
```

_ONNX ì»¨ë²„íŒ…ì€ [soeque1](https://github.com/soeque1)ê»˜ì„œ ë„ì›€ì„ ì£¼ì…¨ìŠµë‹ˆë‹¤._

### Using with MXNet-Gluon

```python
>>> import mxnet as mx
>>> from kobert import get_mxnet_kobert_model
>>> input_id = mx.nd.array([[31, 51, 99], [15, 5, 0]])
>>> input_mask = mx.nd.array([[1, 1, 1], [1, 1, 0]])
>>> token_type_ids = mx.nd.array([[0, 0, 1], [0, 1, 0]])
>>> model, vocab = get_mxnet_kobert_model(use_decoder=False, use_classifier=False)
>>> encoder_layer, pooled_output = model(input_id, token_type_ids)
>>> pooled_output.shape
(2, 768)
>>> vocab
Vocab(size=8002, unk="[UNK]", reserved="['[MASK]', '[SEP]', '[CLS]']")
>>> # Last Encoding Layer
>>> encoder_layer[0]
[[-0.24610372  0.24282135  0.2589539  ... -0.48613444 -0.07305248
   0.07560539]
 [-0.24783105  0.242005    0.25520545 ... -0.48771808 -0.07270523
   0.07536077]
 [-0.24721491  0.241966    0.25606337 ... -0.48743105 -0.07327032
   0.07650219]]
<NDArray 3x768 @cpu(0)>
```

* Naver Sentiment Analysis Fine-Tuning with MXNet
  * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_gluon_kobert.ipynb)

### Tokenizer

* Pretrained [Sentencepiece](https://github.com/google/sentencepiece) tokenizer

```python
>>> from gluonnlp.data import SentencepieceTokenizer
>>> from kobert import get_tokenizer
>>> tok_path = get_tokenizer()
>>> sp  = SentencepieceTokenizer(tok_path)
>>> sp('í•œêµ­ì–´ ëª¨ë¸ì„ ê³µìœ í•©ë‹ˆë‹¤.')
['â–í•œêµ­', 'ì–´', 'â–ëª¨ë¸', 'ì„', 'â–ê³µìœ ', 'í•©ë‹ˆë‹¤', '.']
```

---

## Subtasks

### Naver Sentiment Analysis

* Dataset : <https://github.com/e9t/nsmc>

| Model                                                                                               | Accuracy                                                        |
| --------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| [BERT base multilingual cased](https://github.com/google-research/bert/blob/master/multilingual.md) | 0.875                                                           |
| KoBERT                                                                                              | **[0.901](logs/bert_naver_small_512_news_simple_20190624.txt)** |
| [KoGPT2](https://github.com/SKT-AI/KoGPT2)                                                          | 0.899                                                           |

### KoBERTì™€ CRFë¡œ ë§Œë“  í•œêµ­ì–´ ê°ì²´ëª…ì¸ì‹ê¸°

* <https://github.com/eagle705/pytorch-bert-crf-ner>

```text
ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:  SKTBrainì—ì„œ KoBERT ëª¨ë¸ì„ ê³µê°œí•´ì¤€ ë•ë¶„ì— BERT-CRF ê¸°ë°˜ ê°ì²´ëª…ì¸ì‹ê¸°ë¥¼ ì‰½ê²Œ ê°œë°œí•  ìˆ˜ ìˆì—ˆë‹¤.
len: 40, input_token:['[CLS]', 'â–SK', 'T', 'B', 'ra', 'in', 'ì—ì„œ', 'â–K', 'o', 'B', 'ER', 'T', 'â–ëª¨ë¸', 'ì„', 'â–ê³µê°œ', 'í•´', 'ì¤€', 'â–ë•ë¶„ì—', 'â–B', 'ER', 'T', '-', 'C', 'R', 'F', 'â–ê¸°ë°˜', 'â–', 'ê°', 'ì²´', 'ëª…', 'ì¸', 'ì‹', 'ê¸°ë¥¼', 'â–ì‰½ê²Œ', 'â–ê°œë°œ', 'í• ', 'â–ìˆ˜', 'â–ìˆì—ˆë‹¤', '.', '[SEP]']
len: 40, pred_ner_tag:['[CLS]', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'O', 'O', 'O', 'O', 'O', 'O', 'B-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[SEP]']
decoding_ner_sentence: [CLS] <SKTBrain:ORG>ì—ì„œ <KoBERT:POH> ëª¨ë¸ì„ ê³µê°œí•´ì¤€ ë•ë¶„ì— <BERT-CRF:POH> ê¸°ë°˜ ê°ì²´ëª…ì¸ì‹ê¸°ë¥¼ ì‰½ê²Œ ê°œë°œí•  ìˆ˜ ìˆì—ˆë‹¤.[SEP]
```

### Korean Sentence BERT

* <https://github.com/BM-K/KoSentenceBERT-SKT>

|Model|Cosine Pearson|Cosine Spearman|Euclidean Pearson|Euclidean Spearman|Manhattan Pearson|Manhattan Spearman|Dot Pearson|Dot Spearman|
|:------------------------:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
|NLl|65.05|68.48|68.81|68.18|68.90|68.20|65.22|66.81|
|STS|**80.42**|**79.64**|**77.93**|77.43|**77.92**|77.44|**76.56**|**75.83**|
|STS + NLI|78.81|78.47|77.68|**77.78**|77.71|**77.83**|75.75|75.22|
---

## Release

* v0.2.3
  * support `onnx 1.8.0`
* v0.2.2
  * fix `No module named 'kobert.utils'`
* v0.2.1
  * guide default 'import statements'
* v0.2
  * download large files from `aws s3`
  * rename functions
* v0.1.2
  * Guaranteed compatibility with higher versions of transformers
  * fix pad token index id
* v0.1.1
  * ì‚¬ì „(vocabulary)ê³¼ í† í¬ë‚˜ì´ì € í†µí•©
* v0.1
  * ì´ˆê¸° ëª¨ë¸ ë¦´ë¦¬ì¦ˆ

## Contacts

`KoBERT` ê´€ë ¨ ì´ìŠˆëŠ” [ì´ê³³](https://github.com/SKTBrain/KoBERT/issues)ì— ë“±ë¡í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

## License

`KoBERT`ëŠ” `Apache-2.0` ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ë° ì½”ë“œë¥¼ ì‚¬ìš©í•  ê²½ìš° ë¼ì´ì„ ìŠ¤ ë‚´ìš©ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”. ë¼ì´ì„ ìŠ¤ ì „ë¬¸ì€ `LICENSE` íŒŒì¼ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
